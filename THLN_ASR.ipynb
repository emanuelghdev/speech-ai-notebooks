{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xItZzoCs_-A2"
      },
      "source": [
        "# Presentaci칩n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "游녦 Este cuaderno colab es un recurso generado por Zoraida Callejas y David Griol para la asignatura de [Tecnolog칤as del habla y del Lenguaje Natural](https://masteres.ugr.es/desarrollo-software/docencia/plan-estudios/guia-docente/M52/56/5/23) del M치ster de Desarrollo de Software de la Universidad de Granada. La primera parte del cuaderno es una adaptaci칩n del notebook compartido por [Vaibhav Srivastav](https://colab.research.google.com/github/Vaibhavs10/insanely-fast-whisper/blob/main/infer_faster_whisper_large_v2.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk3SnhKB-jjf"
      },
      "source": [
        "Siguiendo el cuaderno aprender치s a:\n",
        "\n",
        "*   Utilizar **modelos pre-entrenados** basados en *deep learning* para el reconocimiento del habla.\n",
        "*   Subir, descargar, grabar, reproducir y recortar audios.\n",
        "*   Visualizar audios.\n",
        "\n",
        "Puedes ir sigui칠ndolo y ejecutando cada celda de c칩digo secuencialmente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgCW7hjiAY6f"
      },
      "source": [
        "# Instalaciones necesarias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE67hAfXCBBM"
      },
      "source": [
        "El ejemplo que vas a seguir se basa en el modelo <a href=\"https://huggingface.co/openai/whisper-large-v3\">whisper-large-v3</a> de OpenAI. Como se quiere evitar tener que usar grandes recursos CPU/GPU (游뛂游눶) , se propone utilizar una reimplementaci칩n de Whisper mucho m치s r치pida llamada <a href=\"https://pypi.org/project/faster-whisper/\">**Faster Whisper**</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffWKMb3ICz7e"
      },
      "source": [
        "En primer lugar instala faster-whisper:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-3jAUK4or1d"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade -q faster-whisper ipython-autotime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQzzD2JlDShy"
      },
      "source": [
        "En aplicaciones que requieren reconocer el habla, el tiempo de ejecuci칩n suele ser clave. Para saber lo que tarda en ejecutarse cada celda de c칩digo de este cuaderno, carga la extensi칩n \"autotime\". Una vez activada, sabr치s cu치nto ha tardado en ejecutarse cada celda.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqb4IGAnqThp"
      },
      "outputs": [],
      "source": [
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY1FFJgIVmMs"
      },
      "source": [
        "# Descarga un archivo de audio para probar el modelo\n",
        "\n",
        "Prueba en primer lugar el audio del podcast [Lex interviewing Sam Altman](https://www.youtube.com/watch?v=L_Guz73e6fw&t=8s) que est치 subido en hugginface [aqu칤](https://huggingface.co/datasets/reach-vb/random-audios/blob/main/sam_altman_lex_podcast_367.flac)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emKT2iAGqXu5"
      },
      "outputs": [],
      "source": [
        "!wget https://huggingface.co/datasets/reach-vb/random-audios/resolve/main/sam_altman_lex_podcast_367.flac"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNBvUbic_Gwi"
      },
      "source": [
        "# Reconoce habla utilizando el modelo pre-entrenado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygeCJnvsFYyD"
      },
      "source": [
        "Crea una instancia de la clase **WhisperModel** del m칩dulo \"faster_whisper\" (ver la documentaci칩n en [github](https://github.com/SYSTRAN/faster-whisper/blob/master/faster_whisper/transcribe.py)) utilizando los siguientes par치metros en el constructor:\n",
        "* El tama침o del modelo ser치 \"large-v3\" (posibles valores: tiny, tiny.en, base, base.en, small, small.en, medium, medium.en, large-v1, large-v2, large-v3, o large).\n",
        "* El dispositivo donde se ejecutar치 ser치 una GPU usando CUDA (otros posibles valores: \"cpu\", \"cuda\", \"auto\").\n",
        "* Se utilizar치 una representaci칩n de punto flotante de 16 bits para los c치lculos (otros posibles valores documentados [aqu칤](https://opennmt.net/CTranslate2/quantization.html)).\n",
        "\n",
        "Puedes probar a jugar con distintas combinaciones y ver su repercusi칩n los tiempos de reconocimiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzoWzQwboMpk"
      },
      "outputs": [],
      "source": [
        "# Importamos la clase WhisperModel\n",
        "from faster_whisper import WhisperModel\n",
        "\n",
        "# Generamos una instacia denominada mi_modelo\n",
        "mi_modelo = WhisperModel(model_size_or_path=\"large-v3\", device=\"auto\", compute_type=\"float16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2_Q18IPKsEP"
      },
      "source": [
        "Ejecuta el m칠todo **transcribe** para transcribir el archivo de audio. El m칠todo recibe como entrada:\n",
        "* La ruta al archivo de audio.\n",
        "* Beam size. Durante el proceso de decodificaci칩n, el modelo genera una secuencia de posibles tokens (palabras o incluso subpalabras) que puedan venir a continuaci칩n. El algoritmo *beam search* explora estas posibilidades para encontrar la secuencia m치s probable de tokens. El *beam size* determina cu치ntas secuencias posibles utiliza el algoritmo de decodificaci칩n en cada paso.\n",
        "\n",
        "y genera como salida una dupla con:\n",
        "* Los segmentos transcritos.\n",
        "* Informaci칩n sobre el reconocimiento.\n",
        "\n",
        "Puedes probar m치s par치metros de entrada como:\n",
        "* language: el idioma (\"es\", \"en\"...). Si no se indica lo reconoce con los primeros 30 segundos del audio.\n",
        "* Ver m치s en [github](https://github.com/SYSTRAN/faster-whisper/blob/master/faster_whisper/transcribe.py).\n",
        "\n",
        "\n",
        "游낿 Ten paciencia, ejecutar esta celda de c칩digo puede tardar un rato.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xwaBmkMrg5v"
      },
      "outputs": [],
      "source": [
        "# Ejecutamos el m칠todo transcribe (OJO: Puede tardar un rato, seg칰n el tama침o del fichero de audio)\n",
        "segmentos, info = mi_modelo.transcribe(audio=\"Audio Explicativo.mp3\", beam_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrr3SSSlQmTZ"
      },
      "source": [
        "춰Ya puedes comprobar los resultados de la transcripci칩n! 游봅\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Dpm1nGDSMQX"
      },
      "source": [
        "Por ejemplo, la informaci칩n que has recibido en la variable **info**:\n",
        "* language: idioma detectado.\n",
        "* language_probability: con qu칠 probabilidad el modelo considera que se trata de ese idioma en particular.\n",
        "* all_language_probs: probabilidad de todos los idiomas que es capaz de reconocer.\n",
        "* duration: duraci칩n del audio.\n",
        "* duration_after_vad: duraci칩n despu칠s de centrarse 칰nicamente en los segementos del audio que contienen habla (en contraprosici칩n a los silencios o ruidos de fondo).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJNsrP53QjHu"
      },
      "outputs": [],
      "source": [
        "# Imprimimos el idioma detectado (durante los primeros 30s del audio) y con qu칠 probabilidad se trata de ese idioma (con 2 decimales)\n",
        "print(\"El idioma detectado es: '%s', con probabilidad %.2f\" % (info.language, info.language_probability))\n",
        "\n",
        "# Imprimimos la probabilidad asociada a cada idioma (si esta es mayor de 0.000001)\n",
        "for idioma, prob in info.all_language_probs:\n",
        "    if prob > 0.000001:\n",
        "        print(f\"{idioma}, {prob:f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_0fzVeKVQb2"
      },
      "source": [
        "Lo verdaderamente interesante est치 en los segmentos. La se침al de audio se divide en segmentos consecutivos basados en las decisiones del sistema VAD (voice activity detection). Los segmentos con habla se mantienen y procesan. Ahora mismo, tienes almacenados estos segmentos en la variable **segmentos**, junto con informaci칩n como la marca de tiempo de inicio y fin y la transcripci칩n del habla contenida en cada segmento. Para imprimirlos ejecuta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fb6cf60qc1Y"
      },
      "outputs": [],
      "source": [
        "for segmento in segmentos:\n",
        "    print(\"[%.2fs -> %.2fs] %s\" % (segmento.start, segmento.end, segmento.text))\n",
        "    if(segmento.start > 300):\n",
        "       break # Poner breaks en un bucle es muuuuuy mala pr치ctica de programaci칩n 游삒,\n",
        "              # pero nos interesa ver c칩mo ha funcionado m치s que mostrar la larga lista de segmentos completa.\n",
        "              # De todas formas, aqu칤 mandas t칰: si quieres ver la transcripci칩n completa borra el condicional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56R8q8UWa8ns"
      },
      "source": [
        "**춰IMPRESIONANTE!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0mLhXWfan0i"
      },
      "source": [
        "Compara el texto con el audio original, que puedes escuchar [aqu칤](https://cdn-lfs.huggingface.co/repos/96/e4/96e4f69cd112b019dd764318570e47e5fe96de53d8c32a99d745e72d9086e355/b2fd593ce144a8d904cf49a4ed77ed06eb50644a053dddd280c81a3ef94fb60e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27sam_altman_lex_podcast_367.flac%3B+filename%3D%22sam_altman_lex_podcast_367.flac%22%3B&response-content-type=audio%2Fx-flac&Expires=1709058168&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwOTA1ODE2OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy85Ni9lNC85NmU0ZjY5Y2QxMTJiMDE5ZGQ3NjQzMTg1NzBlNDdlNWZlOTZkZTUzZDhjMzJhOTlkNzQ1ZTcyZDkwODZlMzU1L2IyZmQ1OTNjZTE0NGE4ZDkwNGNmNDlhNGVkNzdlZDA2ZWI1MDY0NGEwNTNkZGRkMjgwYzgxYTNlZjk0ZmI2MGU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=QCSGtP08p93enH%7E3lRIhMO6zDqQVfuUW96AAx9%7E%7ErKbGVZV5-13UzaKSOOfu7H4N3JlVhRF-pV4q4lb8Vx1Doc2ZSLjd-JwFKl6VQsPOuHF5iilftzs2g1PjR5mfDN31Bqgcc1Sho14yzIlaPhEh2xXjk-BnYVZtFdkr6RAP9PrWSMkadJEeNgt78STVmmeHMNvn1q1ds2U9m8AUj8oeeKPNGTsqM623kTdshogo9U0HWhHpY4h6hbtpYxjUzL%7EB2WC5I6YK7YlJN9C3VZYlTvU6Pf-zvLc1lJ-yQRDu7J1j7uJ2JqlucvCtuaRfOM4KuRz%7Elcfib5NU%7EZLwdmVaNQ__&Key-Pair-Id=KVTP0A1DKRTAX)...\n",
        "\n",
        "El tiempo de transcripci칩n para esta gran calidad de resultado es incre칤ble, 쯨erdad? 游뱚\n",
        "\n",
        "Puedes reducirlo incluso m치s jugando con los par치metros con los que creaste la instancia de WhisperModel, por ejemplo usando \"int8_float16\" en el par치metro compute_type. Haz la prueba..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCCdXeKbJTjq"
      },
      "source": [
        "# Juega un poco m치s 游띟"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdRJ028Yj8uh"
      },
      "source": [
        "A continuaci칩n se proponen algunas pruebas para trabajar en clase. 칄stas est치n menos guiadas, para que tengas m치s autonom칤a al plantear c칩mo resolverlas; pero no te preocupes: son sencillas y se dan algunas pistas..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJev-OhYjtU2"
      },
      "source": [
        "## Replica lo anterior subiendo tu propio archivo de audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UabebOUMeKX-"
      },
      "source": [
        "Prueba a transcribir tus propios audios mp3. Puede ser un buen momento para ver c칩mo funciona en espa침ol. Lo ideal para no tener tiempos de espera muy largos es que el ejemplo que utilices dure aproximadamente uno o dos minutos.\n",
        "\n",
        "No te cortes copiando, pegando y reutilizando celdas de c칩digo de la parte anterior. Este cuaderno es tuyo para que experimentes todo lo que quieras  游뗵"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLQxcAPDeQfT"
      },
      "outputs": [],
      "source": [
        "# Este trocito de c칩digo hace que salga un bot칩n examinar y puedas subir un fichero.\n",
        "# Para usarlo despu칠s, la ruta ser치 el nombre del archivo, por ejemplo MiAudio.mp3\n",
        "\n",
        "# En su lugar tambi칠n puedes usar el icono de la carpeta del men칰 de la izquierda\n",
        "# y trabajar con la interfaz gr치fica\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a_Xx3XslVGs"
      },
      "source": [
        "## Prueba con un mp3 largo que est칠 colgado en una URL y tengas que recortar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpEIwCHjiD6f"
      },
      "outputs": [],
      "source": [
        "#Aqu칤 tienes un ejemplo con un podcast de RNE\n",
        "\n",
        "!pip install requests pydub\n",
        "\n",
        "import requests\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Funci칩n para descargar el mp3 de una url\n",
        "def download_mp3(url, archivoCompleto):\n",
        "    respuesta = requests.get(url)\n",
        "    with open(archivoCompleto, 'wb') as f:\n",
        "        f.write(respuesta.content)\n",
        "\n",
        "# Funci칩n para cortar el audio entre dos marcas de tiempo\n",
        "def cut_audio(input_path, output_path, start_time, end_time):\n",
        "    audio = AudioSegment.from_file(input_path, format=\"mp3\")\n",
        "    corte = audio[start_time:end_time]\n",
        "    corte.export(output_path, format=\"mp3\")\n",
        "\n",
        "# Descarga y corta. Aqu칤 estamos cortando entre el segundo 10 y el 90\n",
        "url_podcast = 'https://rtve-mediavod-lote3.rtve.es/resources/TE_SYSISIS/mp3/3/1/1708708893513.mp3'\n",
        "download_mp3(url_podcast, 'miaudio.mp3')\n",
        "cut_audio('miaudio.mp3', 'miaudio_cortado.mp3', start_time=10000, end_time=20000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqiFNNe3oEp1"
      },
      "source": [
        "## Prueba ahora a grabar un audio desde tu micr칩fono"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCoBaTpioKZd"
      },
      "outputs": [],
      "source": [
        "# Este trozo de c칩digo est치 basado en: http://blog.syntheticspeech.de/2021/03/22/recording-and-transcribing-a-speech-sample-on-google-colab/\n",
        "\n",
        "from IPython.display import Javascript\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "\n",
        "RECORD = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=>{\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "def graba(ficheroAudio, segundos):\n",
        "  display(Javascript(RECORD))\n",
        "  s = output.eval_js('record(%d)' % (segundos*1000))\n",
        "  b = b64decode(s.split(',')[1])\n",
        "  with open(ficheroAudio,'wb') as f:\n",
        "    f.write(b)\n",
        "  return \"Audio grabado en \"+ficheroAudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "049SR7wOojFF"
      },
      "outputs": [],
      "source": [
        "# Puedes usar la funci칩n \"graba\" como se indica a continuaci칩n.\n",
        "# 丘멆잺 En cuanto ejecutes se pondr치 a grabar durante los segundos que hayas indicado (en el ejemplo 5).\n",
        "# 丘멆잺 Recuerda dar permisos en tu navegador para usar el micr칩fono.\n",
        "import os\n",
        "graba('./migrabacion.wav', 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNMpZSProvLX"
      },
      "outputs": [],
      "source": [
        "# Puedes reproducir el audio:\n",
        "from IPython.display import Audio\n",
        "Audio('migrabacion.wav')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJlWdZ_WoJRg"
      },
      "source": [
        "## Has probado con diferentes audios... prueba ahora diferentes modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGNBl9HDw4V_"
      },
      "source": [
        "Hemos probado un modelo pre-entrenado basado en Whisper.\n",
        "\n",
        "**SpeechBrain** es un toolkit de prop칩sito general sobre tecnolog칤as del habla que provee 100 modelos pre-entrenados de reconocimiento de habla que puedes probar. Te recomendamos que visites su [p치gina web](https://speechbrain.github.io/), pues est치 repleta de recursos sobre procesamiento del habla."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceNKScWFxIb9"
      },
      "outputs": [],
      "source": [
        "# Instalaci칩n de speechbrain\n",
        "BRANCH = 'develop'\n",
        "!python -m pip install git+https://github.com/speechbrain/speechbrain.git@$BRANCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vz-jREryleL"
      },
      "outputs": [],
      "source": [
        "# Accede a un modelo pre-entrenado y util칤zalo para transcribir los archivos de audio con los que has trabajado anteriormente\n",
        "from speechbrain.inference.ASR import EncoderASR\n",
        "\n",
        "#Descomenta este si quieres reconocer el audio en ingl칠s:\n",
        "# from speechbrain.inference.ASR import EncoderDecoderASR\n",
        "# modelo_asr = EncoderDecoderASR.from_hparams(source=\"speechbrain/asr-crdnn-rnnlm-librispeech\", savedir=\"pretrained_models/asr-crdnn-rnnlm-librispeech\")\n",
        "# modelo_asr.transcribe_file(\"migrabacion.wav\")\n",
        "\n",
        "modelo_asr = EncoderASR.from_hparams(source=\"speechbrain/asr-wav2vec2-commonvoice-14-es\", savedir=\"pretrained_models/asr-wav2vec2-commonvoice-14-es\")\n",
        "modelo_asr.transcribe_file(\"miaudio_cortado.mp3\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHCz4o24H67n"
      },
      "source": [
        "# Viendo el sonido..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liD-EHzSIC3-"
      },
      "source": [
        "Existen distintas formas de visualizar el audio, vamos a ver algunas de las m치s importantes. [Aqu칤](https://speechprocessingbook.aalto.fi/Representations/Spectrogram_and_the_STFT.html) tienes material donde se explica c칩mo se interpretan.\n",
        "\n",
        "Primero, vamos a importar las librer칤as necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN6MQYsz7TCr"
      },
      "outputs": [],
      "source": [
        "#importamos las bibliotecas necesarias\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EX-1I2xIOsN"
      },
      "source": [
        "## Espectograma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQhTBONEIHeG"
      },
      "outputs": [],
      "source": [
        "# Function to plot the spectrogram\n",
        "def plot_spectrogram(audio_path):\n",
        "    y, sr = librosa.load(audio_path)\n",
        "    spect = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "    spect_db = librosa.power_to_db(spect, ref=np.max)\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(spect_db, x_axis='time', y_axis='mel', sr=sr, fmax=8000)\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title('Espectograma')\n",
        "    plt.show()\n",
        "\n",
        "# Dibuja el espectograma\n",
        "plot_spectrogram('./migrabacion.wav')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTvJwGvJON9d"
      },
      "source": [
        "## Waveform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1yKpl-iOL_J"
      },
      "outputs": [],
      "source": [
        "def plot_waveform(audio_path):\n",
        "    y, sr = librosa.load(audio_path)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    librosa.display.waveshow(y, sr=sr)\n",
        "    plt.title('Waveform')\n",
        "    plt.show()\n",
        "\n",
        "# Dibuja el waveform\n",
        "plot_waveform('./migrabacion.wav')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WEBwaLHOgAf"
      },
      "source": [
        "## Cromagrama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BN6FZ8UiOj3E"
      },
      "outputs": [],
      "source": [
        "def plot_cromograma(audio_path):\n",
        "    y, sr = librosa.load(audio_path)\n",
        "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    librosa.display.specshow(chroma, y_axis='chroma', x_axis='time', sr=sr)\n",
        "    plt.colorbar()\n",
        "    plt.title('Cromagrama')\n",
        "    plt.show()\n",
        "\n",
        "# Dibuja el cromograma\n",
        "plot_cromograma('./migrabacion.wav')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeyJuDXGOwO-"
      },
      "source": [
        "## Coeficientes cepstrales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9xyTLIJO0tk"
      },
      "outputs": [],
      "source": [
        "def plot_mfcc(audio_path):\n",
        "    y, sr = librosa.load(audio_path)\n",
        "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    librosa.display.specshow(mfccs, x_axis='time', sr=sr)\n",
        "    plt.colorbar()\n",
        "    plt.title('MFCCs')\n",
        "    plt.show()\n",
        "\n",
        "# Dibuja los coeficientes cepstrales\n",
        "plot_mfcc('./migrabacion.wav')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
